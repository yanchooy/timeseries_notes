{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend, Seasonality and Residual Decomposition\n",
    "EDA plotting additive or multiplcative decomposition into trend, seasonal and residual graphs to see how they compare with observed curved. eg. great variance in residual in certain time period suggest unpredicted events in those period. No pattern in seasonal means there is no seasonality.\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "dec_add = seasonal_decompose(df.value, model = 'additive')\n",
    "dec_add.plot()\n",
    "\n",
    "dec_mult = seasonal_decompose(df.value, model = 'multiplicative')\n",
    "dec_mult.plot()\n",
    "\n",
    "```\n",
    "\n",
    "### Stationarity\n",
    "Removing Trend, Irregularity and Seasonality out of data.\n",
    "Consecutive samples of the same size (time period) should have the same \n",
    "- constant mean\n",
    "- constant variance\n",
    "- same covariance between first period and next period\n",
    "\n",
    "### Augmented Dickey-Fuller Test\n",
    "To test for stationarity, based on null hypothesis \n",
    "\n",
    "$H_0 : \\varphi_1 < 1$ - Non- stationary\n",
    "\n",
    "$H_1 : \\varphi_1 = 1$ - Stationary\n",
    "\n",
    "if test statistic < critical value = stationary process\n",
    "\n",
    "```\n",
    "import statsmodels.tsa.stattools as sts\n",
    "sts.adfuller(df.value)\n",
    "```\n",
    "**Results Example:**\n",
    "```\n",
    "(-1.77709238,\n",
    "0.49305483054,\n",
    "18,\n",
    "5002,\n",
    "{'1%': -3.34345\n",
    " '5%': -2.34534,\n",
    " '10%': -2.4544},\n",
    "399923.2342342)\n",
    "\n",
    "(test_statistic, p-value, number of lags, observations used in regression, critical_values, maximised information criteria lower better)\n",
    "```\n",
    "**Result interpretation**\n",
    "- test-statistic is more than critical values\n",
    "- p-value - 40% chance of accepting null, $\\varphi$ is close to <1 and close to 0\n",
    "- time series comes from non-stationary process\n",
    "\n",
    "\n",
    "### Autoregressive model (AR model)\n",
    "A linear model, where current period values $r_t$ are a sum of past outcomes $r_{t-n}$ multipled by a numeric factor $\\varphi$, using a constant benchmark $C$ and including unpredictable shocks from predicted $\\varepsilon$\n",
    "\n",
    "$$\n",
    "r_t = C + \\varphi r_{t-1} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "r_t = C + \\varphi_1 r_{t-1} + \\varphi_2 r_{t-2} + ... + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "where \n",
    "- C is a constant benchmark, usually the first period\n",
    "- $\\varphi$ is a portion of the lagged variable $x_{t-n}$, a coefficient (slope) between -1 to 1\n",
    "- $\\varepsilon_t$ is the residual difference between our prediction for period \"t\" and the correct value. Basically the unpredicatable shocks\n",
    "\n",
    "### Autocorrelation Function (ACF) \n",
    "- Computes correlation of original data and different lag versions of itself\n",
    "- To determine appropriate number of lags as seen from plots\n",
    "- Coefficient(correlation) $\\varphi$ should be significantly above 0 (outside of blue zone) to have an impact in the AR Model\n",
    "- the more lags, the significance zone gets larger, so correlation need to be higher to be significant. This is because the today's price is closer to yesterday's price, sufficiently greater to be of significance as autocorrelation seldom persists the further the distance in time.\n",
    "\n",
    "```\n",
    "import statsmodels.graphics.tsplots as sgt \n",
    "sgt.plot_acf(df.value, zero = False, lags = 40)\n",
    "```\n",
    "\n",
    "### Partial Autocorrelation Function (PACF)\n",
    "- Same as ACF, but compares the direct effect of how one past lag has on the present (2 periods without the in between)\n",
    "\n",
    "`sgt.plot_pacf(df.value, zero = False, lags = 40, alpha = 0.05, method = 'ols')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using percentage change for non-stationary data\n",
    "\n",
    "- use Dickey-fuller test to confirm non-stationarity\n",
    "- Percentage change between price of current period - price of period before\n",
    "\n",
    "$(Price_{t} - Price_{t-1}) /  Price_{t-1} * 100$\n",
    "\n",
    "```\n",
    "df.value.pct_change(1).mul(100)\n",
    "\n",
    "# pct_change default uses 1 time period, returns real number like 0.02\n",
    "# mul(X) multiplies by 100 \n",
    "```\n",
    "\n",
    "- note that normalizing (making numbers into percentage) will not affect model or make it stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR Model\n",
    "\n",
    "- Using PACF to identify initial optimum lags to start modelling\n",
    "- use returns (percentage_change) which is stationary rather than price\n",
    "\n",
    "$$\n",
    "r_t = C + \\varphi r_{t-1} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "```\n",
    "- AR 1 model\n",
    "```\n",
    "model_ar = ARMA(df.value, order = (1, 0)) \n",
    "\n",
    "#order = (AR, MA)\n",
    "#1 is the number of past values (lags) we wish to incorporate into the AR model\n",
    "#0 is that we are not taking any residual values into consideration. MA model\n",
    "\n",
    "results_ar = model_ar.fit()\n",
    "results_ar.summary()\n",
    "\n",
    "```\n",
    "- AR 2 model\n",
    "```\n",
    "model_ar_2 = ARMA(df.value, order = (2, 0)) \n",
    "```\n",
    "\n",
    "- compare constant coef ($C$) and AR1 coef ($\\varphi$)\n",
    "- p-value, if less than 0.05, coef is significantly different from 0 (reject null hypothesis that coef is 0)\n",
    "- p-value, if more than 0.05, coef is not significant different from 0, which means it is close to 0, and $\\varphi x_{t-1}$ has no effect on model.\n",
    "\n",
    "- Checking between models with more lags, log-likelihood should increase, and AIC, BIC, HQIC (known as information criteria) should decrease.\n",
    "\n",
    "- Best model need to satisfy 2 conditions\n",
    "1. Significant p-value for LLR test, with the next lag having non-significant p-value for LLR test \n",
    "2. Significant p-value for lag coefficient, with the next higher lag having non-significant p-value.\n",
    "\n",
    "- So if the last model has a significant p-value for both these conditions, the previous model would be the best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood Test\n",
    "- Compare between AR models with different lags\n",
    "\n",
    "- mod_1 should be the model with lower lag\n",
    "- check for greater log-likelihoods\n",
    "- a function that returns p-value, an acceptable AR model should have a non-significant p-value for LLR\n",
    "\n",
    "```\n",
    "from scipy.stats.distributions import chi2\n",
    "from stats .. import llf\n",
    "\n",
    "def LLR_test(mod_1, mod_2, DF=1):\n",
    "    L1 = mod_1.fit().llf\n",
    "    L2 = mod_2.fit().llf\n",
    "    LR = (2*(L2-L1))\n",
    "    p = chi2.sf(LR, DF).round()\n",
    "    return p\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error residuals\n",
    "\n",
    "Should resemble white noise for a good model\n",
    "-- constant variance, mean, stationarity\n",
    "\n",
    "- Find residual of the good model\n",
    "```\n",
    "df['res_price] = model_ar_7.fit().resid\n",
    "\n",
    "```\n",
    "or if already know results of the model\n",
    "\n",
    "```\n",
    "df['res_price] = results_ar_7.resid\n",
    "\n",
    "```\n",
    "- check variance and mean, if high, suggest that the residuals are not concentrated around the mean. If close to 0, indicate it is like white noise (constant variance, constant mean), and our model is good\n",
    "\n",
    "- Gaussian White noise follows normality, so we can measure up to 3 standard deviations away to define whether variance is too much. Eg. [-3.5%, 3.5%] = 7% variance is a lot for market returns\n",
    "\n",
    "Standard deviation = sqrt(variance)\n",
    "\n",
    "3 std deviation = 3*sqrt(variance)\n",
    "\n",
    "```\n",
    "df.res_price.var()\n",
    "\n",
    "```\n",
    "- run dicky-fuller test to check stationarity i.e. p-value <0.05\n",
    "\n",
    "```\n",
    "sts.adfuller(df.res-price)\n",
    "```\n",
    "\n",
    "- examine ACF of residual prices\n",
    "if within blue zone, it is not significantly different from zero, which fits characterisitc of white noise. if outside of blue zone, suggests there are lags that can predict better.\n",
    "\n",
    "- The further away the lags, the less relevant coefficients even if they seem signficant, this is because markets adjust to shocks, so values far in the past lose relevance.\n",
    "\n",
    "- plot residual prices, it should look like white noise for a good model\n",
    "`df.res_price[1:].plot()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average MA Model\n",
    "\n",
    "To absorb all the unpredicted shocks and move the mean along.\n",
    "- Main difference from AR model is the $\\theta_1$ coefficient and $\\varepsilon_{t-1}$ which takes into account of residual of previous period instead of past outcomes eg. price\n",
    "\n",
    "- Uses ACF to estimate number of lags required for best model instead of PACF, because direct effect of past outcomes does not determine current outcome\n",
    "\n",
    "# MA Model\n",
    "\n",
    "$$\n",
    "r_t = C + \\theta_{1} \\varepsilon_{t-1} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "```\n",
    "- MA 1 model\n",
    "```\n",
    "model_ret_ma_1 = ARMA(df.value, order = (0, 1)) \n",
    "\n",
    "#order = (AR components, MA components)\n",
    "\n",
    "results_ret_ma_1 = model_ret_ma_1.fit()\n",
    "results_ret_ma_1.summary()\n",
    "```\n",
    "- MA 2 model\n",
    "```\n",
    "model_ret_ma_2 = ARMA(df.value, order = (0, 2)) \n",
    "\n",
    "results_ret_ma_2 = model_ret_ma_2.fit()\n",
    "results_ret_ma_2.summary()\n",
    "```\n",
    "- LLR Test p-value to check if model is a signficantly better fit\n",
    "\n",
    "- If the last model has a significant p-value >0.05 for both LLR test and lag coefficient, the previous model would be the best model\n",
    "\n",
    "- Check ACF for lags with higher coefficient and compare with the best fit model. Example, run LLR test between MA(6) vs MA(8), changing degree of freedom to DF=2 as 8 - 6 = 2 more variables.\n",
    "\n",
    "`LLR_test(MA_6, MA_8, DF=2)` \n",
    "\n",
    "- Examine Residuals as per AR process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA model\n",
    "\n",
    "- Combine both AR and MR, allow AR models to calibrate faster and adjust to huge shocks, gives MA models foundation for predicitions than constant.\n",
    "\n",
    "- AR components: Past values and associated coefficient $\\varphi$ \n",
    "- MA components: past residuals and associated coefficient $\\theta_{1}$\n",
    "\n",
    "$$\n",
    "r_t = C + \\varphi r_{t-1} + \\theta_{1} \\varepsilon_{t-1} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "- Start with an over-parameterised ARMA model\n",
    "- However unable to use ACF or PACF for estimating lags, optimal ARMA model would contain fewer components of each type. EG. AR(6) and MA(8) can explain on their own we shouldn't need both lags but lesser. So we can 'half' the components and go for ARMA(3,4) or ARMA(3,3) for simplicity.\n",
    "\n",
    "- ARMA(1,1) model\n",
    "```\n",
    "model_ret_ar_1_ma_1 = ARMA(df.value, order = (1, 1)) \n",
    "\n",
    "#order = (AR components, MA components)\n",
    "\n",
    "results_ret_model_ar_1_ma_1 = model_ret_ar_1_ma_1.fit()\n",
    "results_ret_ar_1_ma_1.summary()\n",
    "```\n",
    "\n",
    "- Compare coefficient p-value and LLR test p-value\n",
    "- Make sure there is nesting between models when using LLR test, otherwise if both models have same degree of freedom eg. DF=6 for both ARMA(1, 5) and ARMA(5, 1), compare based on summary stats LLR (ideal should be higher) and AIC value (ideal should be lower)\n",
    "- Nesting between 2 models ARMA($P_1, Q_1$) and nested ARMA($P_2, Q_2$) must satisfy 3 conditions\n",
    "    1. $P_1 + Q_1 >= P_2 + Q_2$\n",
    "    2. $P_1 >= P_2$\n",
    "    3. $Q_1 >= Q_2$\n",
    "\n",
    "- Investigate residual using ACF to identify better lags and check for possible models as ARMA cannot use ACF (MA model) or PACF (AR Model) to identify initial optimal lag\n",
    "\n",
    "- Make sure best model has residuals are non-significant (basically random) within the first 10 lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "- Adding Integration to ARMA to model non-stationary data because ARMA does not perform well with non-stationary data like prices and index\n",
    "- Not advisable to use ARIMA on converted/stationary data because there is multiple integration causing more computation costs, hard to interpret results if the values are converted because differences become narrower\n",
    "- Integration is like converting price into returns (pct_change) before running ARMA although this is a brute integration\n",
    "- ARIMA(p,d,q)\n",
    "\n",
    "p: AR components\n",
    "\n",
    "d: number of times to integrate the time-series to ensure stationarity\n",
    "\n",
    "q: MA components\n",
    "\n",
    "eg. ARIMA(p, 0, q) = ARMA(p, q)\n",
    "\n",
    "eg. ARIMA(0, 0, q) = MA(q)\n",
    "\n",
    "eg. ARIMA(p, 0, 0) = AR(p)\n",
    "\n",
    "$$\n",
    "\\bigtriangleup P_t = C + \\varphi\\bigtriangleup P_{t-1} + \\theta_{1} \\varepsilon_{t-1} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "$\\bigtriangleup P_t$ is change in value, like returns.\n",
    "\n",
    "eg. for ARIMA(p, 2, q) there are 2 integrations, 1st integration is the change in value, 2nd integration will be the change between change in value\n",
    "\n",
    "- For any integration we lose a single observation like 2x integration will lose 2 days of observation.\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "model_ar_1_i_1_ma_1 = ARIMA(df.value, order=(1,1,1))\n",
    "results_ar_1_i_1_ma_1 = model_ar_1_i_1_ma_1.fit()\n",
    "results_ar_1_i_1_ma_1.summary()\n",
    "```\n",
    "- Note that in summary, there is no coefficent for integration based on the formula. The integration order(d) has no effect on the number of parameters we need to estimate, we are only transforming underlying data to stationary \n",
    "\n",
    "- Check residual ACF to check for optimal lags to model. Note that ACF will fail to compute due to missing observations. so we have use the residuals from the second period(for the case of d=1) i.e. using indexing `[1:]` for d=1 or if d=2 use `[2:]`\n",
    "\n",
    "```\n",
    "df['res_ar_1_i_1_ma_1] = results_ar_1_i_1_ma_1.resid\n",
    "sgt.plot_acf(df.res_ar_1_i_1_ma_1[1:], zero = False, lags =40)\n",
    "```\n",
    "\n",
    "- Comparing LLR and AIC using summary stats\n",
    "\n",
    "```\n",
    "print(\"ARIMA(1,1,1): \\t LL = \", results_ar_1_i_1_ma_1.llf, \"\\t AIR = \", results_ar_1_i_1_ma_1.aic)\n",
    "\n",
    "print(\"ARIMA(1,1,2): \\t LL = \", results_ar_1_i_1_ma_2.llf, \"\\t AIR = \", results_ar_1_i_1_ma_2.aic)\n",
    "```\n",
    "\n",
    "- Run LLR test if models are nested, to rationalise the more complex models\n",
    "```\n",
    "print(\"\\n LLR test p-value = \" + str(LLR_test(model_ar_1_i_1_ma_1, model_ar_1_i_1_ma_3, DF=2)))\n",
    "\n",
    "#degrees of freedom need to adjust if there is more than 1\n",
    "```\n",
    "- Examine ACF residual again to check if they are all insignifacnt or if there exist a better lag model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMAX\n",
    "\n",
    "- Includes consideration of external factors \"MAX\" models.\n",
    "- Good for analysing data as adding external factors may vastly improve modelling performance, but because forecasting requires future data of these external factors, if this is not available, it will not be able to forecast.\n",
    "- ARMAX is the non-integrated version and ARIMAX is the integrated version\n",
    "- Additional $\\beta$ coefficient and  $X$ variable that we are interested in like \n",
    "    - time-varying measuremnet like inflation, index prices\n",
    "    - categorical variable like seperating days of the week\n",
    "    - boolean value like different festive periods\n",
    "    - combination of other external(exogeneous) factors as long as it can affect prices and we have the data available for every period\n",
    "\n",
    "\n",
    "$$\n",
    "\\bigtriangleup P_t = C + \\beta X + \\varphi\\bigtriangleup P_{t-1} + \\theta_{1} \\varepsilon_{t-1} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "$\\bigtriangleup P_t$ is change in value, like returns.\n",
    "\n",
    "- ARIMAX (1,1,1)\n",
    "- Add `exog=array` argument into model, eg. if using S&P500 data we will put `exog=df.spx`\n",
    "\n",
    "```\n",
    "model_ar_1_i_1_ma_1_Xspx = ARIMA(df.value, exog = df.spx order=(1,1,1))\n",
    "results_ar_1_i_1_ma_1_Xspx = model_ar_1_i_1_ma_1_Xspx.fit()\n",
    "results_ar_1_i_1_ma_1_Xspx.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX\n",
    "\n",
    "- Seasonal data with periods that are longer apart eg. every christmas will be 12 months apart\n",
    "\n",
    "- SARIMAX (p, d, q)(P, D, Q, s)\n",
    "\n",
    "(P,D,Q): is the seasonal AR component, seasonal integration, seasonal MA component\n",
    "\n",
    "s: Length of cycle. The number of periods needed to pass before the tendency reappears. If s=1 there is no seasonality. We set s=5 for 5 business day week, and the PQpq values should be lesser than 5 \n",
    "\n",
    "- SARIMAX (1, 0, 2)(2, 0, 1, 5) should have P+Q+p+q = 6 coefficients\n",
    "\n",
    "$$\n",
    "y_t = C + \\varphi y_{t-1} + \\theta_{1} \\varepsilon_{t-1} + \\theta_{1} \\varepsilon_{t-2} + \\phi_{1} (y_{t-5} + \\varphi y_{t-6}) + \\phi_{2} (y_{t-10} + \\varphi y_{t-11}) + \\Theta_{1}(\\varepsilon_{t-5} + \\theta_{1} \\varepsilon_{t-6} + \\theta_{2} \\varepsilon_{t-7}) + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "p : $\\varphi y_{t-1}$\n",
    "\n",
    "q : $\\theta_{1} \\varepsilon_{t-1}   +   \\theta_{1} \\varepsilon_{t-2}$\n",
    "\n",
    "Seasonal P : $\\phi_{1} (y_{t-5} + \\varphi y_{t-6})   +   \\phi_{2} (y_{t-10} + \\varphi y_{t-11})$\n",
    "\n",
    "Seasonal Q : $\\Theta_{1}(\\varepsilon_{t-5} + \\theta_{1} \\varepsilon_{t-6} + \\theta_{2} \\varepsilon_{t-7})$\n",
    "\n",
    "```\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "```\n",
    "- SARIMAX (1,0,1) (2, 0, 1, 5) using S&P500 as exogeneous variable\n",
    "\n",
    "```\n",
    "model_sarimax = SARIMAX(df.value, exog = df.spx order=(1,0,1), seasonal_order = (2,0,1,5))\n",
    "results_sarimax = model_sarimax.fit()\n",
    "results_sarimax.summary()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCH\n",
    "\n",
    "- Autoregressive Conditional Heteroskedasticity model\n",
    "- To measure volatility and exaggerate variance so we can find stability\n",
    "- Measuring variance $\\sigma^2$ dependent on past values' variance\n",
    "- ARCH is used to predict future variance rather than future returns, to expect to see stability in the market but not predict whether the price will go up or down.\n",
    "\n",
    "**ARCH equation**\n",
    "$$\n",
    "Var (r_t | r_{t-1}) = \\sigma^2_t = a_0 + a_1\\varepsilon^2_{t-1} + a_2\\varepsilon^2_{t-2} + ... + a_q\\varepsilon^2_{t-q}\n",
    "$$\n",
    "\n",
    "**ARMA - ARCH model**\n",
    "- uses concept of 2 equations - mean $\\mu$ equation and variance $\\sigma^2$ equation\n",
    "- First fits mean equation to the data and estimates residuals $\\varepsilon_t$\n",
    "- Then based on $\\varepsilon_t$ extracted, estimates the variance\n",
    "\n",
    "1. **mean equation** (combination of 2 equations below)\n",
    "$$\n",
    "r_t = C_0 + \\varphi_1\\mu_{t-1} + \\varepsilon_t\n",
    "$$\n",
    " \n",
    "*mean that follows return and observes some white noise*\n",
    "$$\n",
    "r_t = \\mu_t + \\varepsilon_t\n",
    "$$\n",
    "*mean is a function reflective of past values and past errors with no residual as included in 1st equation*\n",
    "$$\n",
    "\\mu_t = C_0 + \\varphi_1 \\mu_{t-1}\n",
    "$$\n",
    "\n",
    "2. **variance equation** to handle the shocks\n",
    "$$\n",
    "\\sigma^2_t = a_0 + a_1\\varepsilon^2_{t-1} + a_2\\varepsilon^2_{t-2} + ... + a_q\\varepsilon^2_{t-q}\n",
    "$$\n",
    "\n",
    "- Use PACF to observe returns and squared returns to see estimate number of lags required for model\n",
    "\n",
    "ARCH(q) - where q is the number of previous values we include in the model\n",
    "- ARCH(1)\n",
    "    - set volatility vol = 'ARCH' \n",
    "    - set mean = constant if do not know if it is 0, normally assumed as time invariant. Can be set to mean = 'AR' if we know mean is an autoregressive\n",
    "    - set p=1 which is order of the model like ARMA(P), number of periods to take into consideration.\n",
    "\n",
    "```\n",
    "from arch import arch_model\n",
    "\n",
    "model_arch_1 = arch_model(df.returns[1:], mean = 'Constant', vol = 'ARCH', p=1)\n",
    "results_arch_1 = model_arch_1.fit()\n",
    "results_arch_1.summary()\n",
    "\n",
    "results_arch_1 = model_arch_1.fit(update_freq=5)\n",
    "to avoid seeing all iteration results\n",
    "```\n",
    "**Summary stats**\n",
    "\n",
    "- Constant Mean - GARCH Model Results\n",
    "    - Mean Model: Constant Mean\n",
    "    - Vol Model: GARCH\n",
    "    - Distribution: Normal (of the residuals)\n",
    "    - DF model: number of degree of freedoms / number of coefficients which comes from $C_0$ in mean equation, $a_0$ and $a_1$ in variance equation\n",
    "    - R-squared: a measurement of explanatory variation away from the mean. If value is 0, it means residuals are a version of the original data set where every value is decreased by a constant, then there will be no variance and nothing to explain\n",
    "\n",
    "- Mean Model\n",
    "    - mu is a single value because we have a constant mean model\n",
    "    - p-value suggest significance of coefficient of mu, so that we know it is not close to 0\n",
    "\n",
    "- Volatility Model\n",
    "    - omega represents $a_0$ of variance equation\n",
    "    - alpha[1] represents $a_1$ coefficient of squared error terms\n",
    "    - beta[1]\n",
    "\n",
    "**Results**  \n",
    "- Iterations: 6 means that model is light\n",
    "- Log Likelihood should increase, model will auto stop at the best model, if we don't want to see all iterations we can set argument in `fit(update_freq=5)`\n",
    "- Compare Log Likelihood with other models\n",
    "- Ensure all coefficients are significant\n",
    "- However ARCH is used to predict future variance rather than future returns, to expect to see stability in the market but not predict whether the price will go up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GARCH\n",
    "\n",
    "- Generalised Autoregressive Conditional Heteroskedasicity\n",
    "- ARCH equation + Additional conditional variance from the last period to create a benchmark to measure volatility\n",
    "\n",
    "$$\n",
    "Var (y_t | y_{t-1}) = \\Omega + a_1\\varepsilon^2_{t-1} + \\beta_1 \\sigma^2_{t-1}\n",
    "$$\n",
    "- Because additional variance contains same residual in the ARCH equation, only GARCH(1,1) will deliver results while higher orders of more conditional variance will be redundant.\n",
    "- Adding a single past variance gives more predictive power than 11 squared residuals eg. GARCH(1,1) vs ARCH(12) when comparing log-likelihood\n",
    "\n",
    "- GARCH(1,1)\n",
    "```\n",
    "from arch import arch_model\n",
    "\n",
    "model_garch_1_1 = arch_model(df.returns[1:], mean = 'Constant', vol = 'GARCH', p=1, q=1)\n",
    "results_garch_1_1 = model_garch_1_1.fit()\n",
    "results_garch_1_1.summary()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
